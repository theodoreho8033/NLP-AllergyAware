{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ5AaW7vg6yF",
        "outputId": "441cfc84-95f9-477c-926e-69eda47029fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=75a0d0a902c3d1439f01394c608bce6e1d17f24c1b69b0fca0dbbbbf16b780bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import logging\n",
        "import json\n",
        "from datetime import datetime\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "PIY-uT6xdb1R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('tokenizers/punkt/english.pickle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwShpbZog3rK",
        "outputId": "8b6fc9d6-0252-4c4c-f1ca-896ab4b74eba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Error loading tokenizers/punkt/english.pickle: Package\n",
            "[nltk_data]     'tokenizers/punkt/english.pickle' not found in index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/generated_recipes_smollm17b.json'\n",
        "MODEL_NAME = 'SMOL17B'"
      ],
      "metadata": {
        "id": "BQeAw1Q0iK4j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TeXabgQbNgC7"
      },
      "outputs": [],
      "source": [
        "def comparison_metrics(reference, hypothesis, ingredients):\n",
        "  metrics = {}\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "  rouge_scores = scorer.score(reference, hypothesis)\n",
        "\n",
        "  reference_tokens = nltk.word_tokenize(reference.lower())\n",
        "  hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
        "\n",
        "\n",
        "  metrics.update({\n",
        "    'rouge1': rouge_scores['rouge1'].fmeasure,\n",
        "    'rouge2': rouge_scores['rouge2'].fmeasure,\n",
        "    'rougeL': rouge_scores['rougeL'].fmeasure,\n",
        "    'rouge1_precision': rouge_scores['rouge1'].precision,\n",
        "    'rouge1_recall': rouge_scores['rouge1'].recall,\n",
        "    'rouge2_precision': rouge_scores['rouge2'].precision,\n",
        "    'rouge2_recall': rouge_scores['rouge2'].recall,\n",
        "    'rougeL_precision': rouge_scores['rougeL'].precision,\n",
        "    'rougeL_recall': rouge_scores['rougeL'].recall,\n",
        "    })\n",
        "\n",
        "        # BLEU scores\n",
        "  metrics.update({\n",
        "    'bleu1': sentence_bleu([reference_tokens], hypothesis_tokens,\n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function=SmoothingFunction().method1),\n",
        "    'bleu2': sentence_bleu([reference_tokens], hypothesis_tokens,\n",
        "                            weights=(0.5, 0.5, 0, 0),\n",
        "                            smoothing_function=SmoothingFunction().method1),\n",
        "    'bleu3': sentence_bleu([reference_tokens], hypothesis_tokens,\n",
        "                            weights=(0.33, 0.33, 0.33, 0),\n",
        "                            smoothing_function=SmoothingFunction().method1),\n",
        "    'bleu4': sentence_bleu([reference_tokens], hypothesis_tokens,\n",
        "                            smoothing_function=SmoothingFunction().method1),\n",
        "    })\n",
        "  return metrics\n",
        "\n",
        "def single_source_metrics(source, ingredients, field_name):\n",
        "  metrics = {}\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "  if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "  model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  inputs = tokenizer(source, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
        "  inputs['labels'] = inputs['input_ids']\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    perplexity = torch.exp(loss).item()\n",
        "  metrics['perplexity'] = perplexity\n",
        "\n",
        "  if ingredients:\n",
        "    ingredient_list = [ing.strip().lower() for ing in ingredients.split(' ')]\n",
        "    mentioned_ingredients = sum(1 for ing in ingredient_list if ing in source.lower())\n",
        "    metrics['ingredient_coverage'] = mentioned_ingredients / len(ingredient_list)\n",
        "  else:\n",
        "    metrics['ingredient_coverage'] = 0.0\n",
        "\n",
        "        # 4. Step Complexity Score\n",
        "  steps = [s for s in source.split('\\n') if s.strip()]\n",
        "  avg_step_length = np.mean([len(nltk.word_tokenize(step)) for step in steps]) if steps else 0\n",
        "  metrics['step_complexity'] = min(1.0, avg_step_length / 20)  # Normalize to [0,1]\n",
        "\n",
        "        # 5. Recipe Coherence Score (using step order logic)\n",
        "  cooking_verbs = set(['mix', 'stir', 'cook', 'bake', 'boil', 'fry', 'add', 'combine', 'heat', 'pour'])\n",
        "  verb_sequence = []\n",
        "  for step in steps:\n",
        "    tokens = nltk.word_tokenize(step.lower())\n",
        "    verbs = [token for token in tokens if token in cooking_verbs]\n",
        "    if verbs:\n",
        "      verb_sequence.append(verbs[0])\n",
        "\n",
        "        # Check if verbs follow logical cooking order\n",
        "  valid_transitions = {\n",
        "    'mix': ['add', 'pour', 'stir'],\n",
        "    'add': ['mix', 'stir', 'combine'],\n",
        "    'combine': ['mix', 'stir', 'heat'],\n",
        "    'heat': ['cook', 'bake', 'boil', 'fry'],\n",
        "    'stir': ['add', 'pour', 'heat']\n",
        "    }\n",
        "\n",
        "  coherence_score = 0\n",
        "  if len(verb_sequence) > 1:\n",
        "    valid_transitions_count = sum(1 for i in range(len(verb_sequence)-1)\n",
        "                                  if verb_sequence[i+1] in valid_transitions.get(verb_sequence[i], []))\n",
        "    coherence_score = valid_transitions_count / (len(verb_sequence) - 1)\n",
        "  metrics['recipe_coherence'] = coherence_score\n",
        "\n",
        "        # 6. Temperature and Time Specification Score\n",
        "  temp_patterns = r'\\d+\\s*[°℉℃F]'\n",
        "  time_patterns = r'\\d+\\s*(minutes?|mins?|hours?|hrs?)'\n",
        "\n",
        "  has_temp = bool(re.search(temp_patterns, source))\n",
        "  has_time = bool(re.search(time_patterns, source))\n",
        "  metrics['temp_time_score'] = (has_temp + has_time) / 2\n",
        "\n",
        "\n",
        "\n",
        "  final_metrics = {field_name + \"-\" +k: v for k, v in metrics.items()}\n",
        "  return final_metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path = DATA_PATH):\n",
        "  with open(data_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_metrics(data, model_name = MODEL_NAME):\n",
        "  metrics = []\n",
        "  i = 0\n",
        "  for d in data:\n",
        "    c_metrics = comparison_metrics(d['reference_instructions'], d['generation'], d['ingredients'])\n",
        "    true_metrics = single_source_metrics(d['reference_instructions'], d['ingredients'], \"true\")\n",
        "\n",
        "    pred_metrics = single_source_metrics(d['generation'], d['ingredients'], \"pred\")\n",
        "\n",
        "    final_metrics = {**c_metrics, **true_metrics, **pred_metrics}\n",
        "    metrics.append(final_metrics)\n",
        "    print(i)\n",
        "    i += 1\n",
        "  return metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "d-RmVw1KZttr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def average_dicts(dicts):\n",
        "    if not dicts:\n",
        "        return {}\n",
        "\n",
        "    # Initialize defaultdict to sum values and count entries for each key\n",
        "    sums = defaultdict(float)\n",
        "    counts = defaultdict(int)\n",
        "\n",
        "    # Iterate through dictionaries and accumulate sums and counts\n",
        "    for d in dicts:\n",
        "        for key, value in d.items():\n",
        "            sums[key] += value\n",
        "            counts[key] += 1\n",
        "\n",
        "    # Compute averages\n",
        "    averages = {key: sums[key] / counts[key] for key in sums}\n",
        "    return averages"
      ],
      "metadata": {
        "id": "vhclj-VPiwYj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_data()\n",
        "\n",
        "metrics = run_metrics(data)\n",
        "average = average_dicts(metrics)"
      ],
      "metadata": {
        "id": "GVo-lGnRiRX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b9d6c2-cd9e-4430-9426-2c83bf28ebb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"metrics-\" +MODEL_NAME +  \".json\"\n",
        "output_avg = \"avg_metrics-\" +MODEL_NAME +  \".json\"\n",
        "print(output_avg)\n",
        "with open(output_file, \"w\") as f:\n",
        "  json.dump(metrics, f, indent=4)\n",
        "with open(output_avg, \"w\") as f:\n",
        "  json.dump(average, f, indent=4)"
      ],
      "metadata": {
        "id": "I0zG9FyujL1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8145081-110a-437f-cd78-95a720f2f1e7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_metrics-SMOL360.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(average)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjaIzbJW_9ky",
        "outputId": "eb97d90b-8fa9-4704-80c9-24491d63611f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.10886351787996909, 'rouge2': 0.006513438517058858, 'rougeL': 0.05566017553750084, 'rouge1_precision': 0.11274985746060036, 'rouge1_recall': 0.1638964863921114, 'rouge2_precision': 0.0072570171880496515, 'rouge2_recall': 0.00877638983690814, 'rougeL_precision': 0.060782993354623366, 'rougeL_recall': 0.08407971918778841, 'bleu1': 0.06581891478105072, 'bleu2': 0.013308621980433493, 'bleu3': 0.005019753679749801, 'bleu4': 0.0027713637453327788, 'true-perplexity': 39.407653759002685, 'true-ingredient_coverage': 0.584435077808551, 'true-step_complexity': 0.532093880967229, 'true-recipe_coherence': 0.21421724386724397, 'true-temp_time_score': 0.373, 'pred-perplexity': 90.66685971450805, 'pred-ingredient_coverage': 0.15826757596849694, 'pred-step_complexity': 0.9798764912280699, 'pred-recipe_coherence': 0.018, 'pred-temp_time_score': 0.116}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DLWmmftnfs1z"
      }
    }
  ]
}