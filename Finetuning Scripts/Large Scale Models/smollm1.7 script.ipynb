{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900c9dfd-fa83-41b9-8c0a-d99d5c2f773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home1/ranair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home1/ranair/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading tokenizers/punkt/english.pickle: Package\n",
      "[nltk_data]     'tokenizers/punkt/english.pickle' not found in index\n",
      "2024-11-10 15:10:18,136 - INFO - Training configuration: {\n",
      "  \"model_name\": \"HuggingFaceTB/SmolLM-1.7B\",\n",
      "  \"dataset_name\": \"shuyangli94/food-com-recipes-and-user-interactions\",\n",
      "  \"max_length\": 512,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"num_epochs\": 1\n",
      "}\n",
      "2024-11-10 15:10:18,136 - INFO - Starting training pipeline...\n",
      "2024-11-10 15:10:18,136 - INFO - Using device: cuda\n",
      "2024-11-10 15:10:18,137 - INFO - Loading model and tokenizer: HuggingFaceTB/SmolLM-1.7B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc55ce6fe57461a9e00f88d861dc5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 15:10:20,578 - INFO - Added padding token to tokenizer\n",
      "2024-11-10 15:10:31,796 - INFO - Using a subsample of 231637 recipes\n",
      "2024-11-10 15:10:33,356 - WARNING - Error processing recipe: can only join an iterable\n",
      "2024-11-10 15:10:33,740 - INFO - Split dataset into 185308 training ,23164 validation samples and 23164 test samples\n",
      "2024-11-10 15:10:34,103 - INFO - Using 23164 samples for testing\n",
      "2024-11-10 15:10:34,104 - INFO - Tokenizing 185308 texts...\n",
      "2024-11-10 15:11:11,055 - INFO - Tokenizing 23164 texts...\n",
      "2024-11-10 15:11:14,997 - WARNING - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-11-10 15:11:14,999 - INFO - Starting training...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home1/ranair/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='723' max='723' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [723/723 3:37:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.888400</td>\n",
       "      <td>1.867721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.840900</td>\n",
       "      <td>1.845979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/ranair/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home1/ranair/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "2024-11-10 18:48:58,603 - INFO - Saved final model to recipe_model_output_sm1.7/final_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import time\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "#from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM-1.7B\"\n",
    "DATASET_NAME = \"shuyangli94/food-com-recipes-and-user-interactions\"\n",
    "OUTPUT_DIR = \"recipe_model_output_sm1.7\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 1\n",
    "TEST_SAMPLES = 500  # Changed to none for metrics though meanwhile to visualize using this.\n",
    "\n",
    "\n",
    "def setup_logging():\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = f'logs/training_{timestamp}.log'\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    config = {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dataset_name': DATASET_NAME,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    logging.info(f\"Training configuration: {json.dumps(config, indent=2)}\")\n",
    "\n",
    "    return log_filename\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    #logging.info(\"Loading and preprocessing dataset...\")\n",
    "\n",
    "    #try:\n",
    "    #    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    #except ImportError:\n",
    "    #    logging.error(\"Kaggle API not found. Please install it using: pip install kaggle\")\n",
    "    #    raise\n",
    "    #os.environ['KAGGLE_USERNAME'] = 'rahulanilnair'\n",
    "    #os.environ['KAGGLE_KEY'] = '51bb178e6b9c32072b20f0595db9f68a'\n",
    "\n",
    "    #kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "    #if not os.path.exists(kaggle_dir):\n",
    "    #    os.makedirs(kaggle_dir)\n",
    "\n",
    "    #kaggle_config = {\n",
    "    #    \"username\": \"rahulanilnair\",\n",
    "    #    \"key\": \"51bb178e6b9c32072b20f0595db9f68a\"\n",
    "    #}\n",
    "    #kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "    #with open(kaggle_json_path, 'w') as f:\n",
    "    #    json.dump(kaggle_config, f)\n",
    "\n",
    "\n",
    "    #os.chmod(kaggle_json_path, 0o600)\n",
    "\n",
    "    #api = KaggleApi()\n",
    "    #api.authenticate()\n",
    "\n",
    "    #dataset_dir = 'datasets/foodcom'\n",
    "    #if not os.path.exists(os.path.join(dataset_dir, 'PP_recipes.csv')):\n",
    "    #    if not os.path.exists(dataset_dir):\n",
    "    #        os.makedirs(dataset_dir)\n",
    "    #    api.dataset_download_files(DATASET_NAME, path=dataset_dir, unzip=True)\n",
    "    #    logging.info('Dataset downloaded and extracted.')\n",
    "    #else:\n",
    "    #    logging.info('Dataset already exists.')\n",
    "\n",
    "\n",
    "\n",
    "    recipes = pd.read_csv('RAW_recipes.csv',encoding='utf8')\n",
    "    recipes=recipes[['name','ingredients','steps']]\n",
    "    recipes['name'] = recipes['name'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "    recipes['steps']=recipes['steps'].apply(ast.literal_eval)\n",
    "    recipes['ingredients']=recipes['ingredients'].apply(ast.literal_eval)\n",
    "    subsample_size = len(recipes)\n",
    "    recipes = recipes.sample(n=subsample_size, random_state=42)\n",
    "    logging.info(f\"Using a subsample of {len(recipes)} recipes\")\n",
    "\n",
    "    def format_recipe(row):\n",
    "        try:\n",
    "            prompt = f\"Name: {' '.join(row['name'])}\\nIngredients: {' '.join(row['ingredients'])}\\nInstructions:\"\n",
    "            completion = '\\n'.join(row['steps'])\n",
    "            return prompt + completion\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error processing recipe: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    formatted_data = recipes.apply(format_recipe, axis=1)\n",
    "    formatted_data = formatted_data[formatted_data.notna()].tolist()\n",
    "    copy_formatted_data=formatted_data.copy()\n",
    "    train_val_text,test_texts=train_test_split(copy_formatted_data,test_size=0.1,random_state=42)\n",
    "    train_texts,val_texts=train_test_split(train_val_text,test_size=1/9,random_state=42)\n",
    "\n",
    "\n",
    "    logging.info(f\"Split dataset into {len(train_texts)} training ,{len(val_texts)} validation samples and {len(test_texts)} test samples\")\n",
    "    return train_texts, val_texts,test_texts\n",
    "\n",
    "def tokenize_data(texts, tokenizer):\n",
    "    logging.info(f\"Tokenizing {len(texts)} texts...\")\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "class RecipeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "def test_model(model, tokenizer, ingredients,name):\n",
    "    #prompt = f\"\"\"Given recipe name {name} and these ingredients: {ingredients}\n",
    "#Write a detailed recipe with:\n",
    "#1. List of all ingredients with measurements\n",
    "#2. Step-by-step cooking instructions\n",
    "#3. Cooking time and temperature if needed\n",
    "#4. Serving suggestions\n",
    "\n",
    "#Recipe:\n",
    "#\"\"\"\n",
    "    prompt =f\"\"\"You are an expert chef and recipe writer. Given a recipe name and a list of ingredients, create a high-quality, detailed recipe. Follow these steps:\n",
    "\n",
    "1. Analyze the recipe name and ingredients:\n",
    "   - What cuisine or style does this recipe represent?\n",
    "   - Are there any key techniques or cooking methods implied?\n",
    "   - What flavors and textures can you expect?\n",
    "\n",
    "2. Create a comprehensive list of ingredients:\n",
    "   - Include all given ingredients and add any necessary additional ones\n",
    "   - Specify precise measurements for each ingredient\n",
    "   - Consider any prep work needed (e.g., chopped, minced, diced)\n",
    "\n",
    "3. Develop step-by-step cooking instructions:\n",
    "   - Start with any necessary prep work\n",
    "   - Order the steps logically, from start to finish\n",
    "   - Include cooking times and temperatures where applicable\n",
    "   - Describe techniques in detail for clarity\n",
    "\n",
    "4. Consider cooking equipment and methods:\n",
    "   - What kitchen tools or appliances are needed?\n",
    "   - Are there any specific cooking methods to highlight?\n",
    "\n",
    "5. Add finishing touches and serving suggestions:\n",
    "   - How should the dish be plated or presented?\n",
    "   - Are there any garnishes or accompaniments to recommend?\n",
    "   - Suggest an appropriate serving size\n",
    "\n",
    "6. Provide additional tips or variations:\n",
    "   - Offer substitutions for dietary restrictions or preferences\n",
    "   - Suggest ways to adjust the recipe for different serving sizes\n",
    "   - Include any make-ahead or storage instructions\n",
    "\n",
    "Now, create a detailed recipe for: {name}\n",
    "Using these ingredients: {ingredients}\n",
    "\n",
    "Recipe:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    inputs = {k: v.to(device=model.device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.75,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=4,\n",
    "        repetition_penalty=1.3,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    generated_recipe = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    logging.info(\"Generated recipe:\")\n",
    "    logging.info(generated_recipe)\n",
    "\n",
    "    return generated_recipe\n",
    "\n",
    "def calculate_metrics(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference, hypothesis)\n",
    "    try:\n",
    "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "\n",
    "        bleu_scores = {\n",
    "            'bleu1': sentence_bleu([reference_tokens], hypothesis_tokens,\n",
    "                                 weights=(1, 0, 0, 0),\n",
    "                                 smoothing_function=SmoothingFunction().method1),\n",
    "            'bleu2': sentence_bleu([reference_tokens], hypothesis_tokens,\n",
    "                                 weights=(0.5, 0.5, 0, 0),\n",
    "                                 smoothing_function=SmoothingFunction().method1),\n",
    "            'bleu3': sentence_bleu([reference_tokens], hypothesis_tokens,\n",
    "                                 weights=(0.33, 0.33, 0.33, 0),\n",
    "                                 smoothing_function=SmoothingFunction().method1),\n",
    "            'bleu4': sentence_bleu([reference_tokens], hypothesis_tokens,\n",
    "                                 smoothing_function=SmoothingFunction().method1),\n",
    "        }\n",
    "    except:\n",
    "        bleu_scores = {'bleu1': 0, 'bleu2': 0, 'bleu3': 0, 'bleu4': 0}\n",
    "\n",
    "    metrics = {\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'rouge1_precision': rouge_scores['rouge1'].precision,\n",
    "        'rouge1_recall': rouge_scores['rouge1'].recall,\n",
    "        'rouge2_precision': rouge_scores['rouge2'].precision,\n",
    "        'rouge2_recall': rouge_scores['rouge2'].recall,\n",
    "        'rougeL_precision': rouge_scores['rougeL'].precision,\n",
    "        'rougeL_recall': rouge_scores['rougeL'].recall,\n",
    "        **bleu_scores,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, model_name=\"unnamed\"):\n",
    "\n",
    "    model.eval()\n",
    "    metrics = []\n",
    "\n",
    "    num_samples = min(TEST_SAMPLES,len(test_data))\n",
    "    logging.info(f\"Evaluating {model_name} on {num_samples} samples...\")\n",
    "\n",
    "    for recipe in tqdm(test_data[:num_samples]):\n",
    "        parts = recipe.split(\"\\nInstructions:\")\n",
    "        header = parts[0].split(\"\\nIngredients: \")\n",
    "        name = header[0].replace(\"Name: \", \"\")\n",
    "        ingredients = header[1]\n",
    "        reference_instructions = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        generated_recipe = test_model(model, tokenizer, ingredients,name)\n",
    "\n",
    "        scores = calculate_metrics(reference_instructions, generated_recipe)\n",
    "        metrics.append(scores)\n",
    "\n",
    "\n",
    "    avg_metrics = {\n",
    "        metric: sum(m[metric] for m in metrics) / len(metrics)\n",
    "        for metric in metrics[0].keys()\n",
    "    }\n",
    "\n",
    "    logging.info(f\"{model_name} Metrics: {json.dumps(avg_metrics, indent=2)}\")\n",
    "    return avg_metrics\n",
    "\n",
    "def generate_evaluation_report(baseline_metrics, finetuned_metrics, test_cases, model_outputs, training_time):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_dir = f'evaluation_reports/{timestamp}'\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "    with open(f'{report_dir}/evaluation_report.md', 'w') as f:\n",
    "        f.write(\"# Model Evaluation Report\\n\\n\")\n",
    "        f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "        f.write(\"## Model Configuration\\n\\n\")\n",
    "        f.write(f\"- Base Model: {MODEL_NAME}\\n\")\n",
    "        f.write(f\"- Training Epochs: {NUM_EPOCHS}\\n\")\n",
    "        f.write(f\"- Batch Size: {BATCH_SIZE}\\n\")\n",
    "        f.write(f\"- Learning Rate: {LEARNING_RATE}\\n\")\n",
    "        f.write(f\"- Training Time: {training_time:.2f} seconds\\n\\n\")\n",
    "\n",
    "        f.write(\"## Performance Metrics\\n\\n\")\n",
    "        metrics_table = []\n",
    "        headers = [\"Metric\", \"Baseline\", \"Fine-tuned\", \"Improvement\", \"% Change\"]\n",
    "\n",
    "        for metric in baseline_metrics.keys():\n",
    "            baseline = baseline_metrics[metric]\n",
    "            finetuned = finetuned_metrics[metric]\n",
    "            improvement = finetuned - baseline\n",
    "            pct_change = (improvement / baseline) * 100 if baseline != 0 else float('inf')\n",
    "\n",
    "            metrics_table.append([\n",
    "                metric,\n",
    "                f\"{baseline:.4f}\",\n",
    "                f\"{finetuned:.4f}\",\n",
    "                f\"{improvement:+.4f}\",\n",
    "                f\"{pct_change:+.2f}%\"\n",
    "            ])\n",
    "\n",
    "        f.write(tabulate(metrics_table, headers=headers, tablefmt=\"pipe\"))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        plot_metrics_comparison(baseline_metrics, finetuned_metrics, report_dir)\n",
    "        f.write(f\"![Metrics Comparison]({report_dir}/metrics_comparison.png)\\n\\n\")\n",
    "\n",
    "        f.write(\"## Sample Generations Analysis\\n\\n\")\n",
    "        for idx, test_case in enumerate(test_cases):\n",
    "            f.write(f\"### Test Case {idx + 1}\\n\\n\")\n",
    "            f.write(f\"**Recipe Name:** {test_case['name']}\\n\\n\")\n",
    "            f.write(f\"**Ingredients:**\\n{test_case['ingredients']}\\n\\n\")\n",
    "            f.write(f\"**Baseline Generation:**\\n{model_outputs['baseline'][idx]}\\n\\n\")\n",
    "            f.write(f\"**Fine-tuned Generation:**\\n{model_outputs['finetuned'][idx]}\\n\\n\")\n",
    "\n",
    "            case_metrics = calculate_metrics(\n",
    "                model_outputs['baseline'][idx],\n",
    "                model_outputs['finetuned'][idx]\n",
    "            )\n",
    "            f.write(\"**Generation Metrics:**\\n\")\n",
    "            for metric, value in case_metrics.items():\n",
    "                f.write(f\"- {metric}: {value:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"## Additional Analysis\\n\\n\")\n",
    "\n",
    "        baseline_lengths = [len(x.split()) for x in model_outputs['baseline']]\n",
    "        finetuned_lengths = [len(x.split()) for x in model_outputs['finetuned']]\n",
    "\n",
    "        f.write(\"### Generation Length Statistics\\n\\n\")\n",
    "        length_stats = {\n",
    "            \"Model\": [\"Baseline\", \"Fine-tuned\"],\n",
    "            \"Avg Length\": [np.mean(baseline_lengths), np.mean(finetuned_lengths)],\n",
    "            \"Min Length\": [np.min(baseline_lengths), np.min(finetuned_lengths)],\n",
    "            \"Max Length\": [np.max(baseline_lengths), np.max(finetuned_lengths)]\n",
    "        }\n",
    "        f.write(tabulate(pd.DataFrame(length_stats), headers=\"keys\", tablefmt=\"pipe\"))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        f.write(\"### Vocabulary Usage\\n\\n\")\n",
    "        baseline_vocab = set(' '.join(model_outputs['baseline']).split())\n",
    "        finetuned_vocab = set(' '.join(model_outputs['finetuned']).split())\n",
    "        common_words = len(baseline_vocab.intersection(finetuned_vocab))\n",
    "\n",
    "        vocab_stats = {\n",
    "            \"Model\": [\"Baseline\", \"Fine-tuned\"],\n",
    "            \"Unique Words\": [len(baseline_vocab), len(finetuned_vocab)],\n",
    "            \"Common Words\": [common_words, common_words],  # Duplicate value for both rows\n",
    "            \"Model-Specific Words\": [\n",
    "                len(baseline_vocab - finetuned_vocab),\n",
    "                len(finetuned_vocab - baseline_vocab)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        vocab_df = pd.DataFrame(vocab_stats)\n",
    "        f.write(tabulate(vocab_df, headers=\"keys\", tablefmt=\"pipe\"))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "    return f'{report_dir}/evaluation_report.md'\n",
    "\n",
    "def plot_metrics_comparison(baseline_metrics, finetuned_metrics, report_dir):\n",
    "    \"\"\"Generate comparative visualizations of metrics.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    metrics = list(baseline_metrics.keys())\n",
    "    baseline_values = [baseline_metrics[m] for m in metrics]\n",
    "    finetuned_values = [finetuned_metrics[m] for m in metrics]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, baseline_values, width, label='Baseline')\n",
    "    plt.bar(x + width/2, finetuned_values, width, label='Fine-tuned')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Baseline vs Fine-tuned Model Performance')\n",
    "    plt.xticks(x, metrics, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'{report_dir}/metrics_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    log_filename = setup_logging()\n",
    "    logging.info(\"Starting training pipeline...\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "    logging.info(f\"Loading model and tokenizer: {MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        logging.info(\"Added padding token to tokenizer\")\n",
    "\n",
    "    train_texts, val_texts,test_texts = load_and_preprocess_data()\n",
    "\n",
    "    logging.info(f\"Using {len(test_texts)} samples for testing\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    #logging.info(\"Loading baseline model for comparison...\")\n",
    "    #baseline_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    #baseline_model.to(device)\n",
    "    #baseline_metrics = evaluate_model(baseline_model, tokenizer, test_texts, \"Baseline Model\")\n",
    "\n",
    "    train_encodings = tokenize_data(train_texts, tokenizer)\n",
    "    val_encodings = tokenize_data(val_texts, tokenizer)\n",
    "\n",
    "    train_dataset = RecipeDataset(train_encodings)\n",
    "    val_dataset = RecipeDataset(val_encodings)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=1,\n",
    "        eval_steps=350,\n",
    "        save_steps=350,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=8,  # Increased from 4\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,  # Add this\n",
    "        metric_for_best_model=\"eval_loss\",  # Add this\n",
    "        greater_is_better=False,  # Add this\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    )\n",
    "\n",
    "    class RecipeDataCollator(DataCollatorForLanguageModeling):\n",
    "        def __call__(self, examples):\n",
    "            batch = super().__call__(examples)\n",
    "            # Add position IDs\n",
    "            batch[\"position_ids\"] = torch.arange(batch[\"input_ids\"].shape[1])[None, :]\n",
    "            return batch\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=RecipeDataCollator(tokenizer=tokenizer, mlm=False),\n",
    "    )\n",
    "\n",
    "    logging.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    logging.info(f\"Saved final model to {final_model_path}\")\n",
    "\n",
    "    #finetuned_metrics = evaluate_model(model, tokenizer, test_texts, \"Fine-tuned Model\")\n",
    "\n",
    "    #test_cases = [\n",
    "    #    {\n",
    "    #        'name': 'Lemon Garlic Chicken',\n",
    "    #        'ingredients': \"chicken breast, garlic, olive oil, lemon, thyme\"\n",
    "    #    },\n",
    "    #    {\n",
    "    #        'name': 'Classic Vanilla Cake',\n",
    "    #        'ingredients': \"flour, sugar, eggs, butter, vanilla extract\"\n",
    "    #   },\n",
    "    #    {\n",
    "    #        'name': 'Vegetable Fried Rice',\n",
    "    #        'ingredients': \"rice, vegetables, soy sauce, ginger, sesame oil\"\n",
    "    #    }\n",
    "    #]\n",
    "\n",
    "    #model_outputs = {\n",
    "    #    'baseline': [],\n",
    "    #    'finetuned': []\n",
    "    #}\n",
    "\n",
    "    #for case in test_cases:\n",
    "    #    model_outputs['baseline'].append(\n",
    "    #        test_model(baseline_model, tokenizer, case['name'], case['ingredients'])\n",
    "    #    )\n",
    "    #   model_outputs['finetuned'].append(\n",
    "    #       test_model(model, tokenizer, case['name'], case['ingredients'])\n",
    "    #   )\n",
    "\n",
    "    #report_path = generate_evaluation_report(\n",
    "    #    baseline_metrics,\n",
    "    #   finetuned_metrics,\n",
    "    #    test_cases,\n",
    "    #    model_outputs,\n",
    "    #    training_time\n",
    "    #)\n",
    "\n",
    "    #logging.info(f\"Evaluation report generated at: {report_path}\")\n",
    "    #logging.info(f\"Training and testing completed. Check {log_filename} for full logs\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Download all required NLTK data\n",
    "    try:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('punkt_tab')\n",
    "        nltk.download('tokenizers/punkt/english.pickle')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading NLTK data: {e}\")\n",
    "        raise\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc95fbf-e9c2-479d-978d-a0c38c924b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
